﻿﻿﻿﻿﻿﻿﻿# Stock Data Engineering## **Purpose**Personal project to create an automated ELT pipeline using widely accepted data and analytics engineering tools. - [Python](https://www.python.org/)- [SQL](https://en.wikipedia.org/wiki/SQL)- [Docker](https://www.docker.com/)- [Apache Airflow](https://airflow.apache.org/)- [dbt](https://www.getdbt.com/)## **Overview**For the scope of this project, we are only concerned about creating a data model for stocks that are currently listed as a part of the S&P 500. In order to successfully measure a stock's performance, we need to know how the stock is performing on a day-to-day basis in the market, as well as knowing what a company's financial results are from their quarterly earnings reports.### Step 1:  Verifying the S&P 500[There are approximately 20 changes to the S&P 500 each year](https://www.investors.com/etfs-and-funds/sectors/sp-500-stocks-more-than-a-third-get-kicked-out-in-nine-years/), thus, in order to be as automated and programmatic as possible, it is crucial that step 1 be the verification of what stocks make up the S&P 500. To do this, we will incorporate [a web scraping python script](webscraping/) that verifies the current S&P 500 stocks from [Wikipedia](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies) and updates our database if any changes have been made.### Step 2: Extracting & Loading the DataTo get both the daily stock price changes and quarterly earnings reports information, we will utilize the free tier [NASDAQ Data Link API](https://www.nasdaq.com/solutions/data-link-api). - **2A:** We will [call](api-calls/wiki-prices/) the [Wiki-Prices](https://data.nasdaq.com/databases/WIKIP) API for the daily price changes for each S&P 500 listed stock. The API will return the open, close, high, and low price points for a stock during each day.- **2B:** We will [call](api-calls/zacks-fc/) the [Zacks Fundamentals Condensed](https://data.nasdaq.com/databases/ZFA#anchor-fundamentals-condensed-zacks-fc-) API that has quarterly report information for each listed stock.Once the data has been obtained from each API, it will be loaded into a "LANDINGS" schema in a Postgres database.### Step 3: Transforming the DataOnce the data is in our database, we will utilize [dbt](dbt/) for the creation and maintenance of a "NASDAQ" schema that will have fact and dimension tables for any downstream analysis.[NASDAQ Schema](/dbt/NASDAQ_ERD.png)### Step 4: Automation & OrchestrationWe will utilize [Apache Airflow](airflow/) to automate a daily update and orchestrate the 3 steps above in to achieve our final objective of providing downstream analysis of a stock's performance history. Because we are using Docker, we can easily run our Docker Images for each step using [Airflow's DockerOperator](https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html).[Airflow DAG](airflow/AIRFLOW_DAG.png)